# ğŸ” Recurrent Neural Networks (RNN)

---

## âš™ï¸ Experiment 1 â€“ MNIST Classification (Baseline)
A fully connected neural network trained on MNIST to establish a baseline for comparison with RNN-based models.  

ğŸ¯ **Goal:** Understand data representation without sequential modeling.  
ğŸ““ [mnist.ipynb](../Notebooks/mnist.ipynb)

---

## âš™ï¸ Experiment 2 â€“ TensorFlow Callbacks (EarlyStopping & ModelCheckpoint)
Implemented callback functions to optimize training efficiency and prevent overfitting.  

ğŸ¯ **Goal:** Use EarlyStopping and ModelCheckpoint in TensorFlow/Keras.  
ğŸ““ [mnist_callbacks.ipynb](../Notebooks/mnist_callbacks.ipynb)  
ğŸ“˜ [Notes (PDF)](../Docs/TENSORFLOW_CALLBACKS.pdf)

---

## ğŸ§© Experiment 3 â€“ Sentiment Analysis using LSTM
Implemented a Long Short-Term Memory (LSTM) based neural network to perform sentiment analysis on textual data, exploring how sequential models capture contextual dependencies.  

ğŸ““ [Sentiment_Analysis_LSTM.ipynb](../Notebooks/Sentiment_Analysis_LSTM.ipynb)

---

## ğŸ“„ Featured Research Articles

<small>

**01:** ğŸ§¾ *â€œHow Do Recurrent Neural Networks Remember? â€” A Journey Through Timeâ€* &nbsp; ğŸ“˜ [Read PDF](../Docs/How_Do_Recurrent_Neural_Networks_Remember.pdf)  
> ğŸ§  Explores how RNNs retain temporal information and manage gradient flow.  

**02:** ğŸ§¾ *â€œBackpropagation Through Time (BPTT): How RNNs Learn from the Pastâ€* &nbsp; ğŸ“˜ [Read PDF](../Docs/Backpropagation_Through_Time_BPTT.pdf)  
> ğŸ” Explains the unrolling process of RNNs and its computational challenges.  

</small>
