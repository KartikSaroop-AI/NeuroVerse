<p align="center" style="margin: 0; padding: 0;">
  <img 
    src="https://github.com/KartikSaroop-AI/NeuroVerse/blob/main/neuro_versebanner.png.png"
    alt="NeuroVerse Banner"
    width="1000"
    height="300"
    style="display: block; object-fit: cover; border-radius: 10px; box-shadow: 0 3px 10px rgba(0,0,0,0.2);"
  />
</p>

<h1 align="center">ğŸ§  NeuroVerse</h1>
<p align="center">Exploring the universe of neural networks â€” a research-driven journey through Deep Learning, from foundational architectures to cutting-edge AI systems.</p>

<p align="center">
  <img src="https://img.shields.io/badge/Deep%20Learning-Research-orange?style=for-the-badge">
  <img src="https://img.shields.io/badge/TensorFlow-2.x-ff6f00?style=for-the-badge&logo=tensorflow&logoColor=white">
  <img src="https://img.shields.io/badge/PyTorch-1.x-ee4c2c?style=for-the-badge&logo=pytorch&logoColor=white">
  <img src="https://img.shields.io/badge/Jupyter-Notebook-f37626?style=for-the-badge&logo=jupyter">
  <img src="https://img.shields.io/badge/Status-Active-brightgreen?style=for-the-badge">
</p>

---

## ğŸŒŒ About NeuroVerse
**NeuroVerse** is a personal research lab documenting the theory, intuition, and implementation of Deep Learning models.  
It serves as a continuous learning archive â€” blending:
- ğŸ“˜ **Academic papers** summarized in concise, readable form  
- ğŸ’» **Hands-on experiments** with annotated Jupyter notebooks  
- ğŸ§© **Architectural insights** into CNNs, RNNs, LSTMs, and Transformers  
- ğŸ§  **Self-designed projects** to test and visualize deep learning principles  

> â€œA place where every neuron connects curiosity to understanding.â€

---

## ğŸ—‚ï¸ Table of Contents

| No. | Section | Focus Area | Status |
|:---:|:--------|:------------|:--------|
| 1 | [Neural Network Fundamentals](#1--neural-network-fundamentals) | Basic architecture, backpropagation, and activation functions | âœ… Completed |
| 2 | [Convolutional Neural Networks (CNNs)](#2--convolutional-neural-networks-cnns) | Image classification and feature extraction | ğŸŸ¡ In Progress |
| 3 | [Recurrent Neural Networks (RNNs) & LSTMs](#3--recurrent-neural-networks-rnns--lstm) | Sequential modeling, time series, and text data | ğŸŸ¢ Active |
| 4 | [Transformers & Attention](#4--transformers--attention-mechanisms) | Self-attention, BERT, and modern sequence models | ğŸ”œ Upcoming |
| 5 | [Generative Models](#5--generative-models-gans--autoencoders) | GANs, VAEs, and creative AI | ğŸ”œ Upcoming |
| 6 | [Research Articles](#6--research-articles--notes) | Deep learning papers and summaries | ğŸ§¾ Updating |

---

## 1ï¸âƒ£ Neural Network Fundamentals
**Experiments:**
- Perceptron implementation from scratch  
- Backpropagation and gradient descent visualization  
- Role of activation functions in non-linearity  

ğŸ““ [Notebook: Neural_Network_Basics.ipynb](Notebooks/Neural_Network_Basics.ipynb)  
ğŸ“˜ [Article: Understanding Backpropagation.pdf](Docs/Understanding_Backpropagation.pdf)

---

## 2ï¸âƒ£ Convolutional Neural Networks (CNNs)
**Experiments:**
- CNN on CIFAR-10 dataset  
- Visualizing filters, feature maps, and activation layers  
- Transfer learning using pre-trained models (VGG16, ResNet)  

**Articles & Notes**
- **01:** ğŸ§¾ *â€œConvolutional Neural Networks (CNNs)â€* &nbsp; ğŸ“˜ [Read PDF](Docs/CNN.pdf)
- **02:** ğŸ§¾ *â€œFrom Pixels to Perception: Why Convolutional Neural Networks Outperform Traditional ANNs in Vision Tasksâ€* &nbsp; ğŸ“˜ [Read PDF](Docs/CNNimportance.pdf)
- **04:** ğŸ§¾ *â€œPreserving Spatial Information: The Importance of Padding in Convolutional Neural Networksâ€* &nbsp; ğŸ“˜ [Read PDF](Docs/padding.pdf)
- **04:** ğŸ§¾ *â€œFeature Extraction through Convolution Layers: The Core Mechanism of Visual Understandingâ€* &nbsp; ğŸ“˜ [Read PDF](Docs/Convolutionlayers.pdf)
- **05:** ğŸ§¾ *â€œEnhancing Spatial Efficiency: The Importance of Pooling Layers in Convolutional Neural Networksâ€* &nbsp; ğŸ“˜ [Read PDF](Docs/Poolinglayers.pdf)

---

## 3ï¸âƒ£ Recurrent Neural Networks (RNNs) & LSTM
**Experiments:**
- ğŸ§© *Experiment 1:* Neural Networks with MNIST Classification  [mnist.ipynb](mnist.ipynb)
- ğŸ§© *Experiment 2:* TensorFlow Callbacks (EarlyStopping & ModelCheckpoint)  [mnist_callbacks.ipynb](Experiments/mnist_callbacks.ipynb)
- ğŸ§© *Experiment 3:* Sentiment Analysis using LSTM  [Sentiment_analysis_LSTM.ipynb](Sentiment_analysis_LSTM.ipynb)

**Articles & Notes**
- **01:** ğŸ§¾ *â€œHow Do Recurrent Neural Networks Remember? â€” A Journey Through Timeâ€* &nbsp; ğŸ“˜ [Read PDF](How_Do_Recurrent_Neural_Networks_Remember.pdf)
- **02:** ğŸ§¾ *â€œBackpropagation Through Time (BPTT): How RNNs Learn from the Pastâ€* &nbsp; ğŸ“˜ [Read PDF](Backpropagation_Through_Time_BPTT.pdf)
- 

---

## 4ï¸âƒ£ Transformers & Attention Mechanisms
**Focus:**
- Self-attention mechanism  
- Encoder-decoder architecture  
- Fine-tuning BERT for text classification

**Experiments:**

**Articles & Notes:**
- **01:** ğŸ§¾ *â€œTransformers Demystified: Architecture, Components, and the Evolution of Attention Mechanismsâ€* &nbsp; ğŸ“˜ [Read PDF](TRANSFORMERS.pdf)  




---

## 5ï¸âƒ£ Generative Models (GANs & Autoencoders)
**Planned Topics:**
- Variational Autoencoders (VAE)  
- DCGAN for image generation  
- Conditional GANs for style transfer  



**Articles & Notes:**
- **01:** ğŸ§¾ *â€œGenerative Adversarial Networks: Architecture, Working Mechanism, and Practical Applicationsâ€* &nbsp; ğŸ“˜ [Read PDF](Docs/GAN.pdf)


---

## 6ï¸âƒ£ Research Articles & Notes
<small>

**01:** ğŸ§¾ *â€œHow Do Recurrent Neural Networks Remember? â€” A Journey Through Timeâ€* &nbsp; ğŸ“˜ [Read PDF](Docs/How_Do_Recurrent_Neural_Networks_Remember.pdf)  
> ğŸ§  Explores memory retention and gradient dynamics in RNNs.  

**02:** ğŸ§¾ *â€œBackpropagation Through Time (BPTT): How RNNs Learn from the Pastâ€* &nbsp; ğŸ“˜ [Read PDF](Docs/Backpropagation_Through_Time_BPTT.pdf)  
> ğŸ” Discusses temporal unrolling and weight updates in sequence learning.  

**03:** ğŸ§¾ *â€œVisualizing CNNs and Feature Mapsâ€* &nbsp; ğŸ“˜ [Read PDF](Docs/Visualizing_CNN_Feature_Maps.pdf)  
> ğŸ–¼ï¸ Interpreting what convolutional layers actually learn.  

</small>

---

## ğŸ§° Tools & Frameworks
[![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-orange?logo=tensorflow)](https://www.tensorflow.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-red?logo=pytorch)](https://pytorch.org/)
[![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-F7931E?logo=scikit-learn)](https://scikit-learn.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-F37626?logo=jupyter)](https://jupyter.org/)

---

## ğŸ’¬ About This Repository
This repository reflects my **ongoing self-learning and research in Deep Learning**, with a focus on building strong conceptual foundations and replicating experiments from notable papers.

> ğŸ§© *Goal:* To understand the â€œwhyâ€ behind model behavior, not just the â€œhowâ€.

---

â­ *â€œNeuroVerse â€” where every neuron tells a story.â€*

